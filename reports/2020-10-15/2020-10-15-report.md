# Report Week 10/15/20
## Activities/Accomplishments and Concepts/Lessons Learned
 * finished lesson 3 of the fastai course
   * I learned a lot about the different ways machine learning can be dangerous for society
   * I learned how to be thoroughly invested in prioritizing safety/privacy/ethics in order to prevent damage later on
   * preventing negatively impactful models requires a lot of preparation and conscious decision making and ignorance can lead to such models
 * [did some data scraping and image classification model training](https://console.paperspace.com/tewx81sfd/notebook/prl2qabhq)
   * classifying elephants vs hippos vs rhinos (4.65% error rate)
   * classifying cheetah vs leopard vs jaguar-cat (12.94% error rate)
     * used 'jaguar-cat' since 'jaguar' resulted in a bunch of cars
   * lessons learned
     * gained a lot of practice using `DataLoaders`, `cnn_learner`, and other classes/functions from the fast.ai library
     * learned how to scrape images from the internet and prepare them to be used for training an image classification model
     * practiced cleaning image data using the fastai library
     * practiced augmenting data for training
     * was able to choose optimal number of epochs to prevent overfitting by monitoring the validation loss and training loss simultaneously
     * realized that plurality in image search can make a difference!
       * likely would have gained better results if I had used 'elephant' as opposed to 'elephants', etc.
## Issues/Problems
  * The computer came, but I hadn't realized that I might need a wifi adapter
  * I ran into several problems while training my own models which gladly I was able to struggle with and eventually learn from
    * I kept running into the problem of `RuntimeError: DataLoader worker (pid 16560) is killed by signal: Killed`
      * turned out it was due to the fact that I was using a CPU through gradient when what I really needed for the training was a powerful GPU
      * had to create a new notebook on a different machine with Quadro GPU through Gradient, but it worked out!
        * also was having some issues with Gradient and somehow managed to delete my previous notebook (this was 90% my fault) so I had to start from scratch
    * ran into several other problems due to my infamiliarity with the fast.ai library, but eventually was able to figure things out!
  * The results of my models were both fairly underwhelming
    * 4.65% for elephants vs hippos vs rhinos
      * this should have been a fairly easy classification task
      * high error rate perhaps makes sense given my plurality problem
    * 12.95% for cheetah vs leopard vs jaguar-cat
      * this error rate is unacceptably high for most image classification problems
      * granted, most humans wouldn't be able to distinguish between the three cats
    * for both models, there were only 450 total images (150 x 3)
      * given such small datasets, perhaps the results aren't so bad after all
    * looking forward to learning to apply some of the methods for improving training using fast.ai library
  * would like some insight on how CRFs and TSDFs work as well as a few of the other methods mentioned in the paper
## Plans
  * do lesson 4 of the fastai course
  * learn how to tweak hyper-parameters in fast.ai and apply various regularization methods, as well as different cost functions
  * gain more practice working with data and training models
## Article Summaries
[3D Scene Reconstruction from a Single Viewport](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670052.pdf)

The task of 3D Scene Reconstruction from a single colored image was approached using a method of a tree net with splitting of the image into different depth layers. Once the volumetric data is reconstructed, an autoencoder was used to increase the resolution and compress the TSDF (truncated signed distance field). Loss shaping focused the attention of the netowork on the relevant parts of the image for the 3D reconstruction. Perhaps we can experiment with different combinations of methods such as depth reconstruction, segmentation, shape completion, and full scene reconstruction within our simulation environment to stimulate learning and decision making. Because we are using RGB images for our robot, we may be able to employ some of the scene reconstruction methods in this paper to allow for more informed navigation decisions from our robot.

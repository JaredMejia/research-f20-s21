# Report Week 9/24/20
## Activities/Accomplishments and Concepts/Lessons Learned
* Began Practical Deep Learning for Coders course by fast.ai
  * Created a Paperspace account and got started using a Jupyter server on Gradient
  * Completed the first lesson of the course
  * gained some intuition for the myriad of ways that image classification can be used
  * was introduced to a few of the different fastai methods and features
  * gained intuition about choosing a test and validation set and the importance of that choice
  * I'll be tracking the coursework [here](https://github.com/JaredMejia/research-fall-20/tree/master/practical-deep-learning-course/lesson-1-intro)
## Issues/Problems
* sometimes the free servers on Gradient have network issues, but other than that everything went smoothly
* had to move very methodically through the Zeiler paper, but ultimately was able to connect all the dots
## Plans
* make more progress on fast.ai course
* read more research papers to get a better sense of the methods used in deep learning and how we might apply them to our project
## Article Summaries
[Semantic Object Classes in Video: A high-definition ground truth database](http://www0.cs.ucl.ac.uk/staff/G.Brostow/papers/Brostow_2009-PRL.pdf)
Video labeled data can be used to leverage motion cues for recognition, detection, and segmentation, and of course to evaluate existing video algorithms quantitatively. Algorithms which take advantage of spatial and temporal context will likely perform much better than still images. The CamVid Database is one of the only video-based database with per-pixel ground truth for multiple clases, consisting of HD original video sequences, intrinsic calibration, camera pose trajectories, the list of class labels and pseudo-colors, the hand labeled frames, and the stroke logs for the hand labeled frames. InteractLabeler is the software program developed to assist the labelers of the CamVid Database and it can also be used to label image sequences or video files for any other domain in order to produce ground truth for researchers' data in applications relating to object recognition, tracking and segmentation. 

Potentially, we could use transfer learning on a model trained from the CamVid Database in order to train our model to be able to deal with segmentation in our simulated environment, which could possibly allow our robot to make even 'smarter' decisions than those it would make using only a model trained on image classification.

[Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)
(Zeiler et al., 2013) address the problem of a lack of understanding of the features of a deep convolutional network tasked with image recognition. They used a visualization technique the allowed them to determine the function of individual layers within their network throughout training. To do this, they made transformations with a Deconvolutional Netowork, mapping the internal activities of individual layers of the network to input pixel space, resulting in visualizations which highlighted the patterns from the original input that caused the activation in the feature maps. The Deconvolutional Network works almost inversely identically to the Convolutional Network. While the Convolutional Network resembles a structure: convolutional filtering -> rectified linear function -> max pooling, the Deconvolutional Network resembles a structure: max unpooling (using switches from the cnn to approximate the inverse of the pooled maps) -> rectified linear function - convolutional filtering (transposed filters). Projecting each activation from the Deconvolutional Network into pixel space provides visualizations which show the features of each input image which excite a given feature map. This is then used as a tool to compare the features which corresponded with certain layers, and also to witness the progression of a given layer's sensitivity to certain features through training temporally (epoch by epoch). Furthermore, the researchers were able to use these visualizations as they performed ablation on different features of the input images, providing insight to which features were most important for image recognition and eventually leading the researchers to the conclusion that the model indeed was focusing on the object of importance rather than simply the environment or background the object was found in. Repeating these occlusion experiments with different numbers of layers, it was found that the depth of the network was directly related to the model's performance. The results allowed the researchers to diagnose limitations of their initial network's architecture, and hence make modifications to their model in order to optimize its performance. Finally, there was a demonstration of transfer learning of the model trained on ImageNet to other image classification benchmarks, and the model was extremely successful even when limited to a miniscule portion of the new datasets (only 6 images in one case to beat the leading method that had used 10 times as many images!).

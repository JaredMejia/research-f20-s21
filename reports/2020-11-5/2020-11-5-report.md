# Report Week 11/5/20
## Activities/Accomplishments and Concepts/Lessons Learned
- trained some models on mnist
  - 0.0198 error rate for non-pretrained model (4 epochs)
  - 0.0093 error rate for pre-trained model with resnet18 (4 epochs)
  - 0.0069 error rate for pre-trained model with resnet152 (3 epochs)
- 
## Issues/Problems
- kept running into a problem with the dimensions of my data when specifying the model architecture myself rather than using fastai's built in resnet

## Plans

    finish implementing my own classifier for entire MNIST data set
        create multiple variations of model (different loss functions, nonlinearities, etc.)
    do fastai lesson 5
    get practice reading research papers (relevant to project)
    
## Article Summaries
[Learning To Simulate](https://arxiv.org/pdf/1810.02513.pdf)
To generate labelled data in a more efficient manner than human annotation, simulation is becoming a more popular choice. The paper aims to answer the question of what distribution should be used to synthesize data, suggesting that automatically determining simulation parameters can be allow for the performance of a model trained on synthesized data to be maximized. This meta-learning approach aims to learn the parameters of the simulator such that the loss of the model is minimized (bi-level optimization problem). That is, the meta-learner learns how to generate the synthesized data with respect to the main task model which learns to solve the task at hand.

# Report Week 11/5/20
## Activities/Accomplishments and Concepts/Lessons Learned
- trained a bunch of different models on mnist
  - 0.0198 error rate for non-pretrained model (4 epochs)
  - 0.0093 error rate for pre-trained model with resnet18 (4 epochs)
  - 0.0069 error rate for pre-trained model with resnet152 (3 epochs)
  - 0.02 error rate for my very simple 3 layer model (2 epochs)
- gained practice using fastai's `Learner` class
  - using different pretrained models
  - applying different loss functions
- gained practice building my own network architecture with PyTorch's `nn.Sequential`
- read [Policy Gradients in a Nutshell](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d) in order to better understand the model used for the meta learning in the paper
## Issues/Problems
- kept running into a problem with the dimensions of my data when specifying the model architecture myself rather than using fastai's built in resnet
  - gained some help with Professor Clark on how to take a look at built-in architectures in order to emulate certain aspects of their design as I attempt to create my own architectures
- struggled to understand the entirety of the reinforcement learning in the article I read and the paper
  - understood about 80%

## Plans
- continue looking into the 'learning to simulate' and other meta-learning approaches to simulation
- gain a better understanding of reinforcement learning and its many variants
  - meet with Professor Clark for this
- continue the fastai course picking back up with lesson 5
- train some more (image classification) models!
    
## Article Summaries
[Learning To Simulate](https://arxiv.org/pdf/1810.02513.pdf)
To generate labelled data in a more efficient manner than human annotation, simulation is becoming a more popular choice. The paper aims to answer the question of what distribution should be used to synthesize data, suggesting that automatically determining simulation parameters can be allow for the performance of a model trained on synthesized data to be maximized. This meta-learning approach aims to learn the parameters of the simulator such that the loss of the model is minimized (bi-level optimization problem). That is, the meta-learner learns how to generate the synthesized data with respect to the main task model which learns to solve the task at hand. An early version of Unreal Engine 4 is used to simulate traffic scenes and generate a variety of datasets used to train a model on image segmentation.
